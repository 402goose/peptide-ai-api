#!/usr/bin/env python3
"""
Peptide AI - Test Runner CLI

Run persona-based conversation tests and generate feedback.

Usage:
    python testing/run_tests.py --num 10           # Run 10 test conversations
    python testing/run_tests.py --num 100 --full   # Run full 100 iteration test
    python testing/run_tests.py --persona "Sarah"  # Test specific persona
    python testing/run_tests.py --analyze results/test-xxx.json  # Analyze existing results
"""

import asyncio
import argparse
import json
import os
import sys

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from testing.conversation_tester import ConversationTester, TestResult
from testing.personas import PERSONAS, get_persona_by_name


async def run_tests(args):
    """Run test conversations"""
    tester = ConversationTester()

    # Filter personas if specified
    personas = None
    if args.persona:
        try:
            persona = get_persona_by_name(args.persona)
            personas = [persona]
            print(f"Testing with persona: {persona.name}")
        except ValueError as e:
            print(f"Error: {e}")
            print("Available personas:")
            for p in PERSONAS:
                print(f"  - {p.name}")
            return

    print(f"\nğŸ§ª Starting {args.num} test conversations...")
    print(f"   Turns per conversation: {args.turns}")
    print(f"   Personas: {len(personas) if personas else len(PERSONAS)}")
    print()

    result = await tester.run_test_batch(
        num_conversations=args.num,
        turns_per_conversation=args.turns,
        personas=personas
    )

    # Save results
    filepath = tester.save_results(result)

    # Print summary
    tester.print_summary(result)

    # Generate detailed report
    generate_report(result, filepath.replace('.json', '-report.md'))

    return result


def generate_report(result: TestResult, filepath: str):
    """Generate a detailed markdown report"""

    report = f"""# Peptide AI Test Report

**Test ID:** {result.test_id}
**Date:** {result.timestamp}
**Total Conversations:** {result.total_conversations}

## Executive Summary

"""

    return_rate = sum(1 for e in result.evaluations if e.would_persona_return) / len(result.evaluations) if result.evaluations else 0
    report += f"**User Return Rate:** {return_rate*100:.1f}%\n\n"

    report += "### Aggregate Scores\n\n"
    report += "| Metric | Score | Rating |\n"
    report += "|--------|-------|--------|\n"

    for metric, score in sorted(result.aggregate_scores.items(), key=lambda x: x[1], reverse=True):
        rating = "ğŸŸ¢ Excellent" if score >= 8 else "ğŸŸ¡ Good" if score >= 6 else "ğŸ”´ Needs Work"
        report += f"| {metric} | {score:.1f}/10 | {rating} |\n"

    report += "\n## Top Strengths\n\n"
    for s in result.top_strengths:
        report += f"- {s}\n"

    report += "\n## Top Issues to Fix\n\n"
    for i in result.top_issues:
        report += f"- {i}\n"

    report += "\n## Recommended Improvements\n\n"
    for r in result.recommendations:
        report += f"1. {r}\n"

    report += "\n## Detailed Results by Persona\n\n"

    # Group by persona
    persona_results = {}
    for e in result.evaluations:
        if e.persona_name not in persona_results:
            persona_results[e.persona_name] = []
        persona_results[e.persona_name].append(e)

    for persona_name, evals in persona_results.items():
        report += f"### {persona_name}\n\n"

        avg_scores = {}
        for e in evals:
            for metric, score in e.scores.items():
                if metric not in avg_scores:
                    avg_scores[metric] = []
                avg_scores[metric].append(score)

        report += "| Metric | Avg Score |\n"
        report += "|--------|----------|\n"
        for metric, scores in avg_scores.items():
            avg = sum(scores) / len(scores)
            report += f"| {metric} | {avg:.1f} |\n"

        report += "\n**Sample Feedback:**\n"
        for e in evals[:2]:
            report += f"> {e.specific_feedback}\n\n"

    report += "\n---\n*Generated by Peptide AI Testing System*\n"

    with open(filepath, 'w') as f:
        f.write(report)

    print(f"\nğŸ“„ Detailed report saved to: {filepath}")


def analyze_results(filepath: str):
    """Analyze existing test results"""
    with open(filepath) as f:
        data = json.load(f)

    print(f"\nğŸ“Š Analysis of {filepath}")
    print("="*60)

    print(f"\nTest ID: {data['test_id']}")
    print(f"Total Conversations: {data['total_conversations']}")
    print(f"Return Rate: {data.get('return_rate', 0)*100:.1f}%")

    print("\nğŸ“ˆ Score Breakdown:")
    for metric, score in sorted(data['aggregate_scores'].items(), key=lambda x: x[1], reverse=True):
        bar = "â–ˆ" * int(score) + "â–‘" * (10 - int(score))
        status = "âœ…" if score >= 7 else "âš ï¸" if score >= 5 else "âŒ"
        print(f"  {status} {metric:20s} {bar} {score:.1f}")

    print("\nğŸ¯ Priority Fixes (lowest scores):")
    lowest = sorted(data['aggregate_scores'].items(), key=lambda x: x[1])[:3]
    for metric, score in lowest:
        print(f"  â€¢ {metric}: {score:.1f}/10")

    # Analyze by persona
    print("\nğŸ‘¥ Performance by Persona:")
    persona_scores = {}
    for e in data['evaluations']:
        persona = e['persona']
        if persona not in persona_scores:
            persona_scores[persona] = []
        avg = sum(e['scores'].values()) / len(e['scores']) if e['scores'] else 0
        persona_scores[persona].append(avg)

    for persona, scores in sorted(persona_scores.items(), key=lambda x: sum(x[1])/len(x[1])):
        avg = sum(scores) / len(scores)
        status = "âœ…" if avg >= 7 else "âš ï¸" if avg >= 5 else "âŒ"
        print(f"  {status} {persona}: {avg:.1f}/10")


def main():
    parser = argparse.ArgumentParser(description="Peptide AI Test Runner")
    parser.add_argument("--num", type=int, default=10, help="Number of test conversations")
    parser.add_argument("--turns", type=int, default=4, help="Turns per conversation")
    parser.add_argument("--persona", type=str, help="Test specific persona")
    parser.add_argument("--full", action="store_true", help="Run full 100-iteration test")
    parser.add_argument("--analyze", type=str, help="Analyze existing results file")

    args = parser.parse_args()

    if args.analyze:
        analyze_results(args.analyze)
        return

    if args.full:
        args.num = 100

    asyncio.run(run_tests(args))


if __name__ == "__main__":
    main()
